{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I43NxonzOSDg"
      },
      "source": [
        "# Notebook 2 - SQL avec vraies bases de donn√©es\n",
        "## Analyse e-commerce avec PostgreSQL en ligne\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItQV6o4Ojrm"
      },
      "source": [
        "\n",
        "### üéØ Objectifs p√©dagogiques\n",
        "- Connecter Python √† une vraie base de donn√©es PostgreSQL\n",
        "- √âcrire des requ√™tes SQL complexes sur des donn√©es r√©elles\n",
        "- Impl√©menter des analyses RFM avec SQL\n",
        "- Int√©grer SQL et pandas pour des analyses avanc√©es\n",
        "- G√©rer les connexions et la s√©curit√©\n",
        "\n",
        "### üõçÔ∏è Contexte du projet\n",
        "Vous analysez les donn√©es d'un vrai dataset e-commerce (Brazilian E-Commerce Public Dataset) h√©berg√© sur une base PostgreSQL.\n",
        "\n",
        "Objectif : cr√©er une segmentation client√®le pour optimiser les campagnes marketing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K79TBMVvOuoj"
      },
      "source": [
        "## Partie 1 : Connexion √† la base de donn√©es r√©elle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7n18iwPBPe"
      },
      "source": [
        "### üîß Installation et configuration\n",
        "\n",
        "\n",
        "# Installation des d√©pendances\n",
        "\n",
        "\n",
        "```\n",
        "pip install psycopg2-binary sqlalchemy pandas python-dotenv\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_NuY2FHuOhu3"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "#from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbORVz5PXMa"
      },
      "source": [
        "### üåê Base de donn√©es PostgreSQL gratuite (ElephantSQL)\n",
        "\n",
        "**Option 1 : ElephantSQL (20MB gratuit)**\n",
        "1. Cr√©ez un compte sur [elephantsql.com](https://www.elephantsql.com/)\n",
        "2. Cr√©ez une instance \"Tiny Turtle\" (gratuite)\n",
        "3. R√©cup√©rez vos credentials\n",
        "\n",
        "**Option 2 : Supabase (500MB gratuit)**\n",
        "1. Cr√©ez un compte sur [supabase.com](https://supabase.com/)\n",
        "2. Cr√©ez un nouveau projet\n",
        "3. R√©cup√©rez l'URL de connexion PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ytLvCF3fQxRJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "# Configuration de connexion (√† adapter selon votre provider)\n",
        "DATABASE_CONFIG = {\n",
        "\"host\":os.getenv('host'),\n",
        "\"database\":os.getenv('database'),\n",
        "\"user\":os.getenv('user'),\n",
        "\"password\":os.getenv('password'),\n",
        "\"port\":5432,\n",
        "}\n",
        "\n",
        "# Cr√©ation de l'engine SQLAlchemy\n",
        "engine = create_engine(\n",
        "    f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@\"\n",
        "    f\"{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
        ")\n",
        "\n",
        "\n",
        "# Test de connection\n",
        "def test_connection():\n",
        "    try:\n",
        "        df = pd.read_sql(\"SELECT version()\", con=engine)\n",
        "        print(\"Connexion r√©ussie !\")\n",
        "        print(\"Version PostgreSQL :\", df.iloc[0, 0])\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de connexion : {e}\")\n",
        "        return False\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connexion r√©ussie !\n",
            "Version PostgreSQL : PostgreSQL 17.4 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 13.2.0, 64-bit\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXfOgAxGQ3b5"
      },
      "source": [
        "\n",
        "## Partie 2 : Import du dataset e-commerce\n",
        "\n",
        "### üìä Dataset Brazilian E-Commerce\n",
        "Nous utilisons le c√©l√®bre dataset Olist (100k commandes r√©elles).\n",
        "\n",
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2_uVipWkQ_W8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Nettoyage des donn√©es\n",
            "Donn√©es nettoy√©es.\n",
            "Extrait de 10 villes s√©lectionn√©es :\n",
            "['itaueira', 'bueno brandao', 'tres barras', 'maravilhas', 'coronel macedo', 'fervedouro', 'areiopolis', 'montezuma', 'senador guiomard', 'dias d avila']\n",
            "   customer_id     customer_city customer_state\n",
            "0  SIMULATED-0           jussara             GO\n",
            "1  SIMULATED-1              jupi             PE\n",
            "2  SIMULATED-2           jussara             GO\n",
            "3  SIMULATED-3     bueno brandao             MG\n",
            "4  SIMULATED-4  francisco badaro             MG\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n    G√©n√®re des donn√©es e-commerce r√©alistes\\n\\n    √âtapes guid√©es :\\n    1. S√©lectionnez 50 villes br√©siliennes al√©atoirement\\n    2. Cr√©ez des clients avec distribution r√©aliste\\n    3. G√©n√©rez des commandes avec saisonnalit√©\\n    4. Ajoutez des produits avec cat√©gories coh√©rentes\\n    5. Calculez des prix et frais de port bas√©s sur la distance\\n    '"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### üì• Import des donn√©es via API\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import random\n",
        "\n",
        "def import_des_donnees():\n",
        "    # 1. Import CSV en DataFrame\n",
        "    \n",
        "    customers_csv = \"data/olist_customers_dataset.csv\"\n",
        "    products_csv = \"data/olist_products_dataset.csv\"\n",
        "    sellers_csv = \"data/olist_sellers_dataset.csv\"\n",
        "    orders_csv = \"data/olist_orders_dataset.csv\"\n",
        "    order_items_csv = \"data/olist_order_items_dataset.csv\"\n",
        "\n",
        "    # Chargementd des fichiers csv\n",
        "    df_customers = pd.read_csv(customers_csv)\n",
        "    df_products = pd.read_csv(products_csv)\n",
        "    df_sellers = pd.read_csv(sellers_csv)\n",
        "    df_orders = pd.read_csv(orders_csv)\n",
        "    df_order_items = pd.read_csv(order_items_csv)\n",
        "    \n",
        "    return df_customers, df_products, df_sellers, df_orders, df_order_items,\n",
        "\n",
        "def clean_data(df_customers, df_products, df_sellers, df_orders, df_order_items):\n",
        "    print(\" Nettoyage des donn√©es\")\n",
        "\n",
        "    # Renommer certaines colonnes si besoin\n",
        "    df_products = df_products.rename(columns={\"product_category_name\": \"product_category\"})\n",
        "    df_orders = df_orders.rename(columns={\n",
        "    \"order_purchase_timestamp\": \"order_date\",\n",
        "    \"order_delivered_customer_date\": \"order_delivered_date\"\n",
        "})\n",
        "\n",
        "    # Colonnes utiles uniquement\n",
        "    df_customers = df_customers[[\"customer_id\", \"customer_city\", \"customer_state\"]]\n",
        "    df_products = df_products[[\"product_id\", \"product_category\", \"product_weight_g\"]]\n",
        "    df_sellers = df_sellers[[\"seller_id\", \"seller_city\", \"seller_state\"]]\n",
        "    df_orders = df_orders[[\"order_id\", \"customer_id\", \"order_status\", \"order_date\", \"order_delivered_date\"]]\n",
        "    df_order_items = df_order_items[[\"order_id\", \"product_id\", \"seller_id\", \"price\", \"freight_value\"]]\n",
        "\n",
        "\n",
        "\n",
        "    # # Suppression des doublons\n",
        "    # df_customers.drop_duplicates(subset=[\"customer_id\"], inplace=False)\n",
        "    # df_products.drop_duplicates(subset=[\"product_id\"], inplace=False)\n",
        "    # df_sellers.drop_duplicates(subset=[\"seller_id\"], inplace=False)\n",
        "    # df_orders.drop_duplicates(subset=[\"order_id\"], inplace=False)\n",
        "    # df_order_items.drop_duplicates(subset=[\"order_id\", \"product_id\", \"seller_id\"], inplace=False)\n",
        "\n",
        "    print(\"Donn√©es nettoy√©es.\")\n",
        "    return df_customers, df_products, df_sellers, df_orders, df_order_items,\n",
        "\n",
        "\n",
        "# Appel du pipeline complet\n",
        "df_c, df_p, df_s, df_o, df_oi = import_des_donnees()\n",
        "df_c, df_p, df_s, df_o, df_oi = clean_data(df_c, df_p, df_s, df_o, df_oi)\n",
        "\n",
        "# G√©n√©ration de donn√©es e-commerce r√©alistes\n",
        "def generate_ecommerce_data():\n",
        "    # Charger les clients existants\n",
        "    df_customers = pd.read_csv(\"data/olist_customers_dataset.csv\")\n",
        "\n",
        "    # √âtape 1 : Extraire 50 villes br√©siliennes uniques\n",
        "    villes_uniques = df_customers[\"customer_city\"].unique()\n",
        "    villes_selectionnees = random.sample(list(villes_uniques), 50)\n",
        "\n",
        "    print(\"Extrait de 10 villes s√©lectionn√©es :\")\n",
        "    print(villes_selectionnees[:10])\n",
        "\n",
        "    # √âtape 2 : G√©n√©rer des clients fictifs (id + ville + √©tat)\n",
        "    clients_simul√©s = []\n",
        "    for i in range(10000):  # 10 000 clients\n",
        "        ville = random.choice(villes_selectionnees)\n",
        "        sous_df = df_customers[df_customers[\"customer_city\"] == ville]\n",
        "        if not sous_df.empty:\n",
        "            etat = sous_df[\"customer_state\"].iloc[0]\n",
        "            client = {\n",
        "                \"customer_id\": f\"SIMULATED-{i}\",\n",
        "                \"customer_city\": ville,  \n",
        "                \"customer_state\": etat\n",
        "            }\n",
        "            clients_simul√©s.append(client)\n",
        "\n",
        "    df_clients_simul√©s = pd.DataFrame(clients_simul√©s)\n",
        "    print(df_clients_simul√©s.head())\n",
        "\n",
        "    return df_clients_simul√©s\n",
        "\n",
        "# Appel du pipeline complet\n",
        "df_c = generate_ecommerce_data()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    G√©n√®re des donn√©es e-commerce r√©alistes\n",
        "\n",
        "    √âtapes guid√©es :\n",
        "    1. S√©lectionnez 50 villes br√©siliennes al√©atoirement\n",
        "    2. Cr√©ez des clients avec distribution r√©aliste\n",
        "    3. G√©n√©rez des commandes avec saisonnalit√©\n",
        "    4. Ajoutez des produits avec cat√©gories coh√©rentes\n",
        "    5. Calculez des prix et frais de port bas√©s sur la distance\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CREATION DES TABLES SQL\n",
        "\n",
        "from sqlalchemy import create_engine, text\n",
        "import pandas as pd\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def create_tables () :\n",
        "    load_dotenv()\n",
        "    engine = create_engine(\n",
        "        f\"postgresql://{os.getenv('user')}:{os.getenv('password')}@{os.getenv('host')}:{os.getenv('port')}/{os.getenv('database')}\")\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        print(\"Cr√©ation des tables...\")\n",
        "        \n",
        "        create_customers = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS customers (\n",
        "            customer_id VARCHAR(50) PRIMARY KEY,\n",
        "            customer_city VARCHAR(100),\n",
        "            customer_state VARCHAR(2)\n",
        "        );\n",
        "        \"\"\"\n",
        "\n",
        "        create_products = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS products (\n",
        "            product_id VARCHAR(50) PRIMARY KEY,\n",
        "            product_category VARCHAR(100),\n",
        "            product_weight_g NUMERIC\n",
        "        );\n",
        "        \"\"\"\n",
        "\n",
        "        create_sellers = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS sellers (\n",
        "            seller_id VARCHAR(50) PRIMARY KEY,\n",
        "            seller_city VARCHAR(100),\n",
        "            seller_state VARCHAR(2)\n",
        "        );\n",
        "        \"\"\"\n",
        "\n",
        "        create_orders = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS orders (\n",
        "            order_id VARCHAR(50) PRIMARY KEY,\n",
        "            customer_id VARCHAR(50),\n",
        "            order_status VARCHAR(20),\n",
        "            order_date TIMESTAMP,\n",
        "            order_delivered_date TIMESTAMP,\n",
        "            FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n",
        "        );\n",
        "        \"\"\"\n",
        "\n",
        "        create_order_items = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS order_items (\n",
        "            order_id VARCHAR(50),\n",
        "            product_id VARCHAR(50),\n",
        "            seller_id VARCHAR(50),\n",
        "            price NUMERIC,\n",
        "            freight_value NUMERIC,\n",
        "            FOREIGN KEY (order_id) REFERENCES orders(order_id),\n",
        "            FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
        "            FOREIGN KEY (seller_id) REFERENCES sellers(seller_id)\n",
        "        );\n",
        "        \"\"\"\n",
        "\n",
        "        # Ex√©cution des requ√™tes SQL\n",
        "        conn.execute(text(create_customers))\n",
        "        conn.execute(text(create_sellers))\n",
        "        conn.execute(text(create_products))\n",
        "        conn.execute(text(create_orders))\n",
        "        conn.execute(text(create_order_items))\n",
        "    \n",
        "        conn.commit()\n",
        "        print(\"Tables cr√©√©es avec succ√®s.\")\n",
        "\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cr√©ation des tables...\n",
            "Tables cr√©√©es avec succ√®s.\n"
          ]
        }
      ],
      "source": [
        "create_tables()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_postgres(df_customers, df_products, df_sellers, df_orders, df_order_items):\n",
        "    \"\"\"\n",
        "    Envoie les DataFrames dans les tables PostgreSQL (cr√©√©es au pr√©alable)\n",
        "    \"\"\"\n",
        "    print(\"Exportation des donn√©es vers PostgreSQL...\")\n",
        "    load_dotenv()\n",
        "\n",
        "    engine = create_engine(\n",
        "        f\"postgresql://{os.getenv('user')}:{os.getenv('password')}@{os.getenv('host')}:{os.getenv('port')}/{os.getenv('database')}\"\n",
        "    )\n",
        "\n",
        "    # Filtrer les commandes dont le customer_id existe dans la table customers\n",
        "    existing_customers = set(df_c['customer_id'])\n",
        "    df_o = df_o[df_o['customer_id'].isin(existing_customers)]\n",
        "\n",
        "\n",
        "    # Insertion dans les tables cr√©√©es \n",
        "    df_customers.to_sql(\"customers\", engine, index=False, if_exists=\"append\")\n",
        "    df_products.to_sql(\"products\", engine, index=False, if_exists=\"append\")\n",
        "    df_sellers.to_sql(\"sellers\", engine, index=False, if_exists=\"append\")\n",
        "    df_orders.to_sql(\"orders\", engine, index=False, if_exists=\"append\")\n",
        "    df_order_items.to_sql(\"order_items\", engine, index=False, if_exists=\"append\")\n",
        "\n",
        "    print(\"Donn√©es ins√©r√©es avec succ√®s.\")\n",
        "\n",
        "    # V√©rification\n",
        "    df_export = pd.read_sql(\"SELECT * FROM products LIMIT 5;\", engine)\n",
        "    print(\"Aper√ßu de la table 'products' :\")\n",
        "    print(df_export)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQ_BY-QT4dO"
      },
      "source": [
        "## Partie 3 : Requ√™tes SQL avanc√©es\n",
        "\n",
        "\n",
        "### üîç Analyses SQL √† impl√©menter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdl5RNOBUAV2"
      },
      "source": [
        "#### 1. Analyse RFM (R√©cence, Fr√©quence, Montant)\n",
        "```sql\n",
        "-- Votre d√©fi : Calculer les m√©triques RFM pour chaque client\n",
        "WITH customer_metrics AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        c.customer_state,\n",
        "        -- R√©cence : jours depuis dernier achat\n",
        "        -- Fr√©quence : nombre de commandes\n",
        "        -- Montant : total d√©pens√©\n",
        "        \n",
        "        -- Compl√©tez cette requ√™te CTE\n",
        "        \n",
        "    FROM customers c\n",
        "    JOIN orders o ON c.customer_id = o.customer_id\n",
        "    JOIN order_items oi ON o.order_id = oi.order_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    GROUP BY c.customer_id, c.customer_state\n",
        ")\n",
        "\n",
        "-- Cr√©ez les segments RFM (Champions, Loyaux, √Ä risque, etc.)\n",
        "SELECT\n",
        "    customer_id,\n",
        "    customer_state,\n",
        "    recency_score,\n",
        "    frequency_score,\n",
        "    monetary_score,\n",
        "    CASE\n",
        "        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'\n",
        "        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n",
        "        -- Ajoutez les autres segments\n",
        "        ELSE 'Others'\n",
        "    END as customer_segment\n",
        "FROM customer_metrics;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWF9rpZSUMp5"
      },
      "outputs": [],
      "source": [
        "#### 2. Analyse g√©ographique des ventes\n",
        "\n",
        "def geographic_sales_analysis():\n",
        "    \"\"\"\n",
        "    Analysez les performances par √©tat/r√©gion\n",
        "\n",
        "    Requ√™tes √† √©crire :\n",
        "    1. Top 10 des √©tats par CA\n",
        "    2. Croissance MoM par r√©gion\n",
        "    3. Taux de conversion par ville\n",
        "    4. Distance moyenne vendeur-acheteur\n",
        "    \"\"\"\n",
        "\n",
        "    query_top_states = \"\"\"\n",
        "    -- Votre requ√™te SQL ici\n",
        "    -- Utilisez des JOINs et GROUP BY\n",
        "    -- Calculez le CA, nombre de commandes, panier moyen\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(query_top_states, engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OE-UHLKY8-K"
      },
      "source": [
        "#### 3. Analyse temporelle et saisonnalit√©\n",
        "```sql\n",
        "-- D√©tectez les patterns saisonniers\n",
        "SELECT\n",
        "    EXTRACT(YEAR FROM order_date) as year,\n",
        "    EXTRACT(MONTH FROM order_date) as month,\n",
        "    EXTRACT(DOW FROM order_date) as day_of_week,\n",
        "    COUNT(*) as order_count,\n",
        "    SUM(price + freight_value) as total_revenue,\n",
        "    AVG(price + freight_value) as avg_order_value\n",
        "FROM orders o\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE order_status = 'delivered'\n",
        "GROUP BY ROLLUP(\n",
        "    EXTRACT(YEAR FROM order_date),\n",
        "    EXTRACT(MONTH FROM order_date),\n",
        "    EXTRACT(DOW FROM order_date)\n",
        ")\n",
        "ORDER BY year, month, day_of_week;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq43e3mfZC8d"
      },
      "source": [
        "## Partie 4 : Analyse pr√©dictive avec SQL\n",
        "\n",
        "### üîÆ Mod√®les simples en SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5mfxFoaL2K"
      },
      "outputs": [],
      "source": [
        "#### 1. Pr√©diction de churn\n",
        "\n",
        "def churn_prediction_sql():\n",
        "    \"\"\"\n",
        "    Identifiez les clients √† risque de churn\n",
        "\n",
        "    Indicateurs :\n",
        "    - Pas d'achat depuis X jours\n",
        "    - Baisse de fr√©quence d'achat\n",
        "    - Diminution du panier moyen\n",
        "    - Changement de comportement g√©ographique\n",
        "    \"\"\"\n",
        "\n",
        "    churn_query = \"\"\"\n",
        "    WITH customer_activity AS (\n",
        "        -- Calculez les m√©triques d'activit√© r√©cente\n",
        "        -- Comparez avec l'historique du client\n",
        "        -- Scorez le risque de churn\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        days_since_last_order,\n",
        "        order_frequency_trend,\n",
        "        monetary_trend,\n",
        "        churn_risk_score,\n",
        "        CASE\n",
        "            WHEN churn_risk_score > 0.7 THEN 'High Risk'\n",
        "            WHEN churn_risk_score > 0.4 THEN 'Medium Risk'\n",
        "            ELSE 'Low Risk'\n",
        "        END as churn_segment\n",
        "    FROM customer_activity;\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(churn_query, engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2D1PDraVu4"
      },
      "source": [
        "#### 2. Recommandations produits\n",
        "```sql\n",
        "-- Market Basket Analysis simplifi√©\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        oi1.product_id as product_a,\n",
        "        oi2.product_id as product_b,\n",
        "        COUNT(*) as co_purchase_count\n",
        "    FROM order_items oi1\n",
        "    JOIN order_items oi2 ON oi1.order_id = oi2.order_id\n",
        "    WHERE oi1.product_id != oi2.product_id\n",
        "    GROUP BY oi1.product_id, oi2.product_id\n",
        "    HAVING COUNT(*) >= 10  -- Seuil minimum\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    product_a,\n",
        "    product_b,\n",
        "    co_purchase_count,\n",
        "    co_purchase_count::float / total_a.count as confidence\n",
        "FROM product_pairs pp\n",
        "JOIN (\n",
        "    SELECT product_id, COUNT(*) as count\n",
        "    FROM order_items\n",
        "    GROUP BY product_id\n",
        ") total_a ON pp.product_a = total_a.product_id\n",
        "ORDER BY confidence DESC;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYkj8ItabH-"
      },
      "source": [
        "## Partie 5 : Int√©gration avec les APIs m√©t√©o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4CU6SNEfNXb"
      },
      "source": [
        "### üå§Ô∏è Croisement donn√©es m√©t√©o/ventes\n",
        "```python\n",
        "def weather_sales_correlation():\n",
        "    \"\"\"\n",
        "    Correlez vos donn√©es m√©t√©o du Notebook 1 avec les ventes\n",
        "    \n",
        "    Hypoth√®ses √† tester :\n",
        "    1. Les ventes de certaines cat√©gories augmentent-elles avec la pluie ?\n",
        "    2. Y a-t-il un impact de la temp√©rature sur les achats ?\n",
        "    3. Les livraisons sont-elles impact√©es par la m√©t√©o ?\n",
        "    \"\"\"\n",
        "    \n",
        "    # R√©cup√©rez les donn√©es m√©t√©o historiques pour les villes br√©siliennes\n",
        "    weather_query = \"\"\"\n",
        "    SELECT DISTINCT customer_city, customer_state\n",
        "    FROM customers\n",
        "    WHERE customer_state IN ('SP', 'RJ', 'MG', 'RS', 'SC')\n",
        "    ORDER BY customer_city;\n",
        "    \"\"\"\n",
        "    \n",
        "    cities = pd.read_sql(weather_query, engine)\n",
        "    \n",
        "    # Int√©grez avec l'API m√©t√©o\n",
        "    # Analysez les corr√©lations\n",
        "    \n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHG9k_5PfZXd"
      },
      "source": [
        "### üìä Dashboard g√©o-temporel\n",
        "```python\n",
        "def create_geotemporal_dashboard():\n",
        "    \"\"\"\n",
        "    Cr√©ez un dashboard interactif combinant :\n",
        "    - Carte des ventes par r√©gion\n",
        "    - √âvolution temporelle avec m√©t√©o\n",
        "    - Segments clients g√©olocalis√©s\n",
        "    - Pr√©dictions par zone g√©ographique\n",
        "    \"\"\"\n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIuD-IVfnxW"
      },
      "source": [
        "---\n",
        "## üèÜ Livrables finaux\n",
        "\n",
        "### üìà Rapport d'analyse complet\n",
        "1. **Segmentation RFM (Recency, Frenquency, Monetary) ** : 5-7 segments avec caract√©ristiques\n",
        "2. **Analyse g√©ographique**  : Performances par r√©gion + recommandations\n",
        "3. **Pr√©dictions churn** : Liste des clients √† risque + actions\n",
        "4. **Recommandations produits** : Top 10 des associations\n",
        "5. **Impact m√©t√©o** : Corr√©lations significatives identifi√©es\n",
        "\n",
        "### üöÄ Pipeline automatis√©\n",
        "```python\n",
        "def automated_analysis_pipeline():\n",
        "    \"\"\"\n",
        "    Pipeline qui :\n",
        "    1. Se connecte √† la DB\n",
        "    2. Ex√©cute toutes les analyses\n",
        "    3. Met √† jour les segments clients\n",
        "    4. G√©n√®re le rapport automatiquement\n",
        "    5. Envoie des alertes si n√©cessaire\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wynvmdtNftwf"
      },
      "source": [
        "## üéì Auto-√©valuation\n",
        "\n",
        "- [ ] **Connexion DB** : PostgreSQL fonctionnelle\n",
        "- [ ] **Requ√™tes complexes** : JOINs, CTEs, fonctions analytiques\n",
        "- [ ] **Gestion des erreurs** : Connexions robustes\n",
        "- [ ] **Performance** : Requ√™tes optimis√©es avec index\n",
        "- [ ] **Int√©gration** : SQL + Python + APIs\n",
        "- [ ] **Insights actionables** : Recommandations business claires\n",
        "\n",
        "### üîó Pr√©paration au Notebook 3\n",
        "Le prochain notebook portera sur NoSQL (MongoDB) avec des donn√©es de r√©seaux sociaux et d'IoT, en temps r√©el.\n",
        "\n",
        "### üí° Bases de donn√©es alternatives\n",
        "- **PlanetScale** : MySQL serverless gratuit\n",
        "- **MongoDB Atlas** : 512MB gratuit\n",
        "- **FaunaDB** : Base multi-mod√®le gratuite\n",
        "- **Hasura Cloud** : GraphQL + PostgreSQL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
